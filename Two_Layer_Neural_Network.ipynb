{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment_9_Two_Layer_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXmQGNBWwzS1",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "- In this assignment, you will build a two layer neural network for classification from scratch using only numpy.\n",
        "- Please refer to videos on Backpropagation and one reference material shared in additional resources for the understanding required to solve this assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCZeHgK-xAy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Some functions required for testing \"\"\"\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_model(D, H, C):\n",
        "  il = keras.layers.Input(shape=(D,))\n",
        "  hl = keras.layers.Dense(H, activation = 'relu')(il)\n",
        "  ol = keras.layers.Dense(C, activation = 'softmax')(hl)\n",
        "  model = keras.models.Model(inputs = [il], outputs = [ol])\n",
        "\n",
        "  rng = np.random.RandomState(2020)\n",
        "  model.layers[1].set_weights([rng.rand(D * H).reshape(D, H), rng.rand(H, )])\n",
        "  model.layers[2].set_weights([rng.rand(H * C).reshape(H, C), rng.rand(C, )])\n",
        "  return model\n",
        "\n",
        "def create_inputs(N, D):\n",
        "  rng = np.random.RandomState(2020)\n",
        "  return rng.rand(N * D).reshape(N, D)\n",
        "\n",
        "def set_weights_from_model(tln, test_net):\n",
        "  tln.params['W1'] = test_net.layers[1].get_weights()[0]\n",
        "  tln.params['b1'] = test_net.layers[1].get_weights()[1]\n",
        "  tln.params['W2'] = test_net.layers[2].get_weights()[0]\n",
        "  tln.params['b2'] = test_net.layers[2].get_weights()[1]\n",
        "  return tln\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9OoL62wOux9t",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class TwoLayerNet(object):\n",
        "    \"\"\"\n",
        "    A two-layer fully-connected neural network. The net has an input dimension of\n",
        "    D, a hidden layer dimension of H, and performs classification over C classes.\n",
        "    We train the network with a softmax loss function and L2 regularization on the\n",
        "    weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
        "    connected layer.\n",
        "\n",
        "    In other words, the network has the following architecture:\n",
        "\n",
        "    input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "    The outputs of the second fully-connected layer are the scores for each class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
        "        \"\"\"\n",
        "        Initialize the model. \n",
        "        Weights are initialized to small random values and\n",
        "        biases are initialized to zero. \n",
        "        Weights and biases are stored in the\n",
        "        variable self.params, which is a dictionary with the following keys:\n",
        "\n",
        "        W1: First layer weights; has shape (D, H)\n",
        "        b1: First layer biases; has shape (H,)\n",
        "        W2: Second layer weights; has shape (H, C)\n",
        "        b2: Second layer biases; has shape (C,)\n",
        "\n",
        "        Inputs:\n",
        "        - input_size: The dimension N of the input data.\n",
        "        - hidden_size: The number of neurons H in the hidden layer.\n",
        "        - output_size: The number of classes C.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        D = input_size\n",
        "        H = hidden_size\n",
        "        C = output_size\n",
        "        self.params['W1'] = np.random.normal(0, std, D*H).reshape(D, H)\n",
        "        self.params['b1'] = np.zeros(H)\n",
        "        self.params['W2'] = np.random.normal(0, std, H*C).reshape(H, C)\n",
        "        self.params['b2'] = np.zeros(C)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ScjBCeTwzS7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac375927-0b00-41fc-f1e8-12f99f78b732"
      },
      "source": [
        "\"\"\" Test Cases for Initialization\"\"\"\n",
        "tln = TwoLayerNet(2, 3, 2)\n",
        "assert tln.params['W1'].shape == (2, 3)\n",
        "assert tln.params['b1'].shape == (3, )\n",
        "assert tln.params['W2'].shape == (3, 2)\n",
        "assert tln.params['b2'].shape == (2, )\n",
        "print('Test passed', '\\U0001F44D')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed üëç\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kh8LRCT9voz1",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(TwoLayerNet):\n",
        "\n",
        "    def forward(self, X):\n",
        "      \"\"\"\n",
        "      Compute the output of a full forward pass of the network.\n",
        "      \n",
        "      First apply weights W1 and biases b1 on inputs and then apply relu non-linearity.\n",
        "      Then apply weights W2 and biases b2 on hidden layer values and then apply softmax non-linearity to get the output\n",
        "      \n",
        "      Inputs:\n",
        "      - X : Input data of shape (N, D). Each X[i] is a training sample\n",
        "      \n",
        "      Outputs:\n",
        "      - y_out : numpy array with Outputs of shape (N, C)\n",
        "      \n",
        "      \"\"\"\n",
        "      # z is value of node before applying activation function\n",
        "      # a is value of node after applying activation function\n",
        "      z1 = X@self.params['W1'] + self.params['b1']\n",
        "      a1 = np.maximum(z1, 0)\n",
        "      z2 = a1@self.params['W2'] + self.params['b2']\n",
        "      a2_num = np.e**(z2)\n",
        "      a2_denom = np.sum(a2_num, axis = 1)\n",
        "      a2 = np.zeros(a2_num.shape)\n",
        "      for i in range(z2.shape[0]):\n",
        "        a2[i, :] = a2_num[i, :] / a2_denom[i]\n",
        "      y_out = a2\n",
        "      \n",
        "      \n",
        "      return y_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJeXhkRZwzTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "491e015d-226d-4852-8d77-98eaa45be811"
      },
      "source": [
        "\"\"\"Test Cases for Forward pass\"\"\"\n",
        "tln = TwoLayerNet(2, 4, 2)\n",
        "test_net = create_model(2, 4, 2)\n",
        "tln = set_weights_from_model(tln, test_net)\n",
        "X = create_inputs(4, 2)\n",
        "y_forward = tln.forward(X)\n",
        "assert y_forward.shape == (4, 2)\n",
        "assert np.all(np.isclose(y_forward, test_net.predict(X), atol = 0.0001))\n",
        "print('Test passed', '\\U0001F44D')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed üëç\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbOelhQCwzTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(TwoLayerNet):\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this two-layer network to predict labels for\n",
        "        data points. For each data point we predict scores for each of the C\n",
        "        classes, and assign each data point to the class with the highest score.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
        "          classify.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
        "          the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
        "          to have class c, where 0 <= c < C.\n",
        "        \"\"\"\n",
        "        y_pred = None\n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO: Implement this function; it should be VERY simple!                #\n",
        "        ###########################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        y_pred = None\n",
        "        y_out = self.forward(X)\n",
        "        y_pred = np.argmax(y_out, axis = 1)\n",
        "        \n",
        "        return y_pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAJuwv5bwzTK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa297abf-1a9f-4ce1-c5a4-75851afc4f3c"
      },
      "source": [
        "\"\"\" Test Cases for predict\"\"\"\n",
        "tln = TwoLayerNet(2, 4, 2)\n",
        "test_net = create_model(2, 4, 2)\n",
        "tln = set_weights_from_model(tln, test_net)\n",
        "X = create_inputs(4, 2)\n",
        "y_pred = tln.predict(X)\n",
        "test_pred = np.argmax(test_net.predict(X), axis = 1)\n",
        "assert np.all(np.isclose(y_pred, test_pred, atol = 0.01))\n",
        "print('Test passed', '\\U0001F44D')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed üëç\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tabhV02TwzTO",
        "colab_type": "text"
      },
      "source": [
        "#### Loss\n",
        "Note: <br>\n",
        "$L = -\\sum{t_i \\log{p_i}}$ <br>\n",
        "where $p_i$ is probability score predicted by model. <br>\n",
        "$t_i = 1$ for the true class $i$ and $t_i = 0$ for all other classes for a particular sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mO3-DSCwzTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(TwoLayerNet):    \n",
        "    def loss(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute the loss and gradients for a two layer fully connected neural\n",
        "        network.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
        "        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
        "          an integer in the range 0 <= y[i] < C.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
        "        the score for class c on input X[i].\n",
        "\n",
        "        If y is not None, instead return a tuple of:\n",
        "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
        "          samples. (This is the mean loss over N samples)\n",
        "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
        "          with respect to the loss function; has the same keys as self.params.\n",
        "        \"\"\"\n",
        "\n",
        "        #############################################################################\n",
        "        # TODO: Perform the forward pass, computing the class scores for the input. #\n",
        "        # Store the result in the scores variable, which should be an array of      #\n",
        "        # shape (N, C).                                                             #\n",
        "        #############################################################################\n",
        "        \n",
        "        \n",
        "        \n",
        "        W1, b1 = self.params['W1'], self.params['b1']\n",
        "        W2, b2 = self.params['W2'], self.params['b2']\n",
        "        N, D = X.shape\n",
        "        H, C = W2.shape\n",
        "        \n",
        "        \n",
        "        \n",
        "        # # Compute the loss\n",
        "        loss = None\n",
        "\n",
        "        #############################################################################\n",
        "        # TODO: Finish the forward pass, and compute the loss. This should include  #\n",
        "        # both the data loss and L2 regularization for W1 and W2. Store the result  #\n",
        "        # in the variable loss, which should be a scalar. Use the Categorical       #\n",
        "        # Cross Entropy loss.                                                       #\n",
        "        #############################################################################\n",
        "        scores=None\n",
        "      \n",
        "        z1 = X@self.params['W1'] + self.params['b1']\n",
        "        a1 = np.maximum(z1, 0)\n",
        "        z2 = a1@self.params['W2'] + self.params['b2']\n",
        "        a2_num = np.e**(z2)\n",
        "        a2_denom = np.sum(a2_num, axis = 1)\n",
        "        a2 = np.zeros(a2_num.shape)\n",
        "        for i in range(z2.shape[0]):\n",
        "          a2[i, :] = a2_num[i, :] / a2_denom[i]\n",
        "        y_out = a2\n",
        "        scores = y_out\n",
        "        loss = None\n",
        "        loss_samples = np.zeros(N)\n",
        "        for i in range(N):\n",
        "          loss_samples[i] = -np.log(scores[i, y[i]])\n",
        "        loss = np.mean(loss_samples)\n",
        "        \n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        grads = {}\n",
        "\n",
        "        da2 = np.zeros((N, C))\n",
        "        for i in range(N):\n",
        "          da2[i, y[i]] = -1/a2[i, y[i]]\n",
        "        \n",
        "\n",
        "        da2_num = np.zeros((N, C))\n",
        "        da2_denom = np.zeros(N)\n",
        "        for i in range(N):\n",
        "          da2_num[i] = (1/a2_denom[i])*da2[i,: ]\n",
        "          da2_denom[i] = np.sum((-a2_num[i, :]/a2_denom[i]**2)*da2[i, :])\n",
        "\n",
        "        for i in range(N):\n",
        "          da2_num[i] += da2_denom[i]\n",
        "        \n",
        "        dz2 = a2_num * da2_num\n",
        "        da1 = dz2@self.params['W2'].T\n",
        "        dW2 = a1.T @ dz2\n",
        "        db2 = np.sum(dz2, axis = 0)\n",
        "        \n",
        "        \n",
        "        drelu = np.vectorize(lambda x: 1 if x>0 else 0)\n",
        "        dz1 = drelu(a1) * da1\n",
        "        dW1 = X.T @ dz1\n",
        "        db1 = np.sum(dz1, axis=0)\n",
        "\n",
        "        grads['W1'] = dW1/N\n",
        "        grads['b1'] = db1/N\n",
        "        grads['W2'] = dW2/N\n",
        "        grads['b2'] = db2/N\n",
        "        \n",
        "\n",
        "\n",
        "  \n",
        "        return loss, grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn1tAYrpwzTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70caf3a5-a8aa-4b92-9250-26b68f548d17"
      },
      "source": [
        "\"\"\" Tests for loss and gradient computation \"\"\"\n",
        "### First compute loss and gradients using keras\n",
        "model = create_model(2, 4, 2)\n",
        "X = create_inputs(4, 2)\n",
        "y = np.array([0, 1, 1, 0])\n",
        "y_onehot = keras.utils.to_categorical(y, 2)\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3, momentum=0.0, nesterov=False, name=\"SGD\")\n",
        "loss_fn = keras.losses.CategoricalCrossentropy()\n",
        "batch_size = 4\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X, y_onehot))\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_out = model(x_batch_train, training = True)\n",
        "\n",
        "      ## Compute loss value for this minibatch\n",
        "      loss_value = loss_fn(y_batch_train, y_out)\n",
        "    \n",
        "    grads_model = {}\n",
        "    grads_model['W1'], grads_model['b1'], grads_model['W2'], grads_model['b2'] = [dw.numpy() for dw in tape.gradient(loss_value, model.trainable_weights)]\n",
        "\n",
        "### Compute loss and gradients using TwoLayerNet\n",
        "tln = TwoLayerNet(2, 4, 2)\n",
        "tln = set_weights_from_model(tln, model)\n",
        "loss, grads_tln = tln.loss(X, y)\n",
        "\n",
        "#### Now match\n",
        "## Loss should be correctly computed\n",
        "assert np.isclose(loss, loss_value.numpy(), atol = 0.0001)\n",
        "\n",
        "## Gradients should be correctly computed\n",
        "assert np.all(np.isclose(grads_tln['W1'], grads_model['W1'], atol = 0.0001))\n",
        "assert np.all(np.isclose(grads_tln['b1'], grads_model['b1'], atol = 0.0001))\n",
        "assert np.all(np.isclose(grads_tln['W2'], grads_model['W2'], atol = 0.0001))\n",
        "assert np.all(np.isclose(grads_tln['b2'], grads_model['b2'], atol = 0.0001))\n",
        "\n",
        "print('Test passed', '\\U0001F44D')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed üëç\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANJB4KJni0ME",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyIYI5btwzTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(TwoLayerNet):\n",
        "    def train(self, X, y, X_val, y_val,\n",
        "              learning_rate=1e-3, num_iters=100,\n",
        "              batch_size=200, verbose=False):\n",
        "        \"\"\"\n",
        "        Train this neural network using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) giving training data.\n",
        "        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
        "          X[i] has label c, where 0 <= c < C.\n",
        "        - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
        "        - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
        "        - learning_rate: Scalar giving learning rate for optimization.\n",
        "        - num_iters: Number of steps to take when optimizing.\n",
        "        - batch_size: Number of training examples to use per step.\n",
        "        \"\"\"\n",
        "        num_train = X.shape[0]\n",
        "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
        "\n",
        "        # Use SGD to optimize the parameters in self.model\n",
        "        loss_history = []\n",
        "        train_acc_history = []\n",
        "        val_acc_history = []\n",
        "        shuffled_indices = np.arange(X.shape[0])\n",
        "        np.random.shuffle(shuffled_indices)\n",
        "        dataset_size = X.shape[0]\n",
        "        X_shuffled = X[shuffled_indices]\n",
        "        y_shuffled = y[shuffled_indices]\n",
        "\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO: Create a random minibatch of training data and labels, storing  #\n",
        "            # them in X_batch and y_batch respectively.                             #\n",
        "            #########################################################################\n",
        "\n",
        "            \n",
        "            start = (num_iters * batch_size)%dataset_size\n",
        "            X_batch = X_shuffled[start: start + batch_size]\n",
        "            y_batch = y_shuffled[start: start + batch_size]\n",
        "\n",
        "            \n",
        "            \n",
        "            # Compute loss and gradients using the current minibatch\n",
        "            loss, grads = self.loss(X_batch, y=y_batch)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO: Use the gradients in the grads dictionary to update the         #\n",
        "            # parameters of the network (stored in the dictionary self.params)      #\n",
        "            # using stochastic gradient descent. You'll need to use the gradients   #\n",
        "            # stored in the grads dictionary defined above.                         #\n",
        "            #########################################################################\n",
        "\n",
        "            \n",
        "            self.params['W1'] -= learning_rate * grads['W1']\n",
        "            self.params['W2'] -= learning_rate * grads['W2']\n",
        "            self.params['b1'] -= learning_rate * grads['b1']\n",
        "            self.params['b2'] -= learning_rate * grads['b2']\n",
        "            \n",
        "\n",
        "            # Every epoch, check train and val accuracy\n",
        "            if it % iterations_per_epoch == 0:\n",
        "                # Check accuracy\n",
        "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
        "                val_acc = (self.predict(X_val) == y_val).mean()\n",
        "                train_acc_history.append(train_acc)\n",
        "                val_acc_history.append(val_acc)\n",
        "\n",
        "                # Decay learning rate\n",
        "                # learning_rate *= learning_rate_decay\n",
        "\n",
        "        return {\n",
        "          'loss_history': loss_history,\n",
        "          'train_acc_history': train_acc_history,\n",
        "          'val_acc_history': val_acc_history,\n",
        "        }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYEfUA1nmCju",
        "colab_type": "text"
      },
      "source": [
        "### Using these networks on datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsACefMSmFX0",
        "colab_type": "text"
      },
      "source": [
        "### XOR\n",
        "Use TwoLayerNet to train the XOR function discussed in the class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeUsLKYEwzTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "17170474-6dfd-4008-943f-2abb5cc91654"
      },
      "source": [
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "xor_net = TwoLayerNet(2, 5, 2)\n",
        "h = xor_net.train(X_xor, y_xor, X_xor, y_xor, batch_size = 4, num_iters = 20000, learning_rate = 0.01)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(h['loss_history'])\n",
        "print(h['loss_history'][-10:])\n",
        "xor_net.predict(X_xor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.02451713794335396, 0.02450683114707762, 0.024496537590373634, 0.024487866108697885, 0.024498315340995905, 0.024491297756757183, 0.024481009158599822, 0.02447073376992434, 0.02446047155327872, 0.024461078149452205]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3D4GjcImFbW",
        "colab_type": "text"
      },
      "source": [
        "### Iris\n",
        "Use TwoLayerNet to train the iris dataset. Choose 120 samples randomly for training and the rest for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSjLcJT1mX-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "5945b82c-a981-4de9-9305-71a8bfe7c575"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "random_indices = np.arange(150)\n",
        "np.random.shuffle(random_indices)\n",
        "X_iris = iris['data'][random_indices[:120]]\n",
        "y_iris = iris['target'][random_indices[:120]]\n",
        "X_iris_val = iris['data'][random_indices[120:]]\n",
        "y_iris_val = iris['target'][random_indices[120:]]\n",
        "iris_net = TwoLayerNet(4, 64, 3)\n",
        "h = iris_net.train(X_iris, y_iris, X_iris_val, y_iris_val, num_iters = 1000, batch_size = 20, learning_rate = 0.05)\n",
        "plt.plot(h['loss_history'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f361ea24a20>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbFD9EXlmaDI",
        "colab_type": "text"
      },
      "source": [
        "### Advanced\n",
        "Add weight regularization to the loss and rewrite backprop part of TwoLayerNet. <br>\n",
        "Train using some datasets and see if regularized network performs better than its older counterpart.\n",
        "<br>\n",
        "The expression for loss with regularization is as follows - <br>\n",
        "$L = -\\sum{t_i \\log{p_i}} + \\lambda(|w_1|^2 + |w_2|^2)$ <br>\n",
        "$\\lambda$ is a tunable hyper-parameter  denoting strength of regularization. <br>\n",
        "If it is too high, network will struggle to fit, and if it is too low, network will overfit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBAJhyB9oB8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}